# gricean-pragmatics

- **Naturalness**: LLMs will generate surprisal scores as a proxy to text naturalness for each sentence in a minimal pair, which reflect how unexpected a sentence is, given the preceding context. We hypothesize that if LLMs show pragmatic sensitivity, LLMs should assign a lower surprisal score to the intended implied meaning in an appropriate context. For example, for Utterance: “Anna rolls her eyes and says, he’s a real genius. What does Anna intend to mean?” (a) He is very smart (b) He is not smart at all, a LLM’s success means that surprisals difference between Utterance and (b) should be smaller than Utterance and (a).
- **Sensitivity to different Shades of Meaning (SSM)**: The (cosine) similarity will be calculated to examine the extent to which LLMs can tease apart pragmatically enriched (implied) meaning from the entailed meaning and other layers of meanings. Similarities between Alex was not unaware of the issue and Alex was slightly aware of the issue would be compared to the similarity score between Alex was aware of the issue and Alex was slightly aware of the issue. We hypothesize that pragmatic relations are not as strong as entailment but still stronger than neutral relations, leading to the following rank of similarity: entailment > implication > neutral. We are aware that cosine similarity is symmetric, whereas the utterance relations here are not. We plan to calculate weighted cosine similarity, in combination with prompt engineering techniques to directly probe LLMs’ labels for relations.
- **Pragmatic Reasoning Chains (PRC)**: PRC involves evaluating the LLM's ability to follow multi-step pragmatic reasoning. For example, given a conversational context that requires understanding a scalar implicature ("some" implying "not all"), an LLM is probed using prompt engineering to generate the reasoning steps of how the “not all” inference is derived. LLM is assessed for its ability to generate the appropriate reasoning steps that align with formal pragmatics’ proposal (c.f., REASONING STEPS).
- **Implicature Recovery Rate (IRR)**: This metric would measure how well LLMs can recover implicatures after introducing deliberate noise or ambiguity into the conversation. For instance, after a model generates an ambiguous or misleading response, the IRR would assess its ability to recognize and correct the implicature in the following turns. This could be used to evaluate robustness in handling and resolving implicature-related ambiguities. The IRR is calculated as the ratio of successfully recovered implicatures to the total number of implicature errors introduced. For example, in the context “It's getting late.”, a possible Implicature is “The speaker wants to leave”. Other possible implicatures include “Let’s stop working now” or “Let’s go to bed” and so on. Suppose that the speaker’s intention is such-and-such, LLM’s initial response (with noise) might be “Yes, it’s almost midnight, we should start the movie”. This can be an initial error. The initial response is incorrect or off-mark from the expected implicature. Here, instead of recognizing that the statement implies a desire to leave, the LLM suggests starting an activity. Now we introduce a recovery phase: following the initial incorrect response, we provide additional context or a prompt that should help the LLM recover the correct implicature. This could involve rephrasing the context or introducing a follow-up utterance that makes the expected implicature more explicit. For instance, we provide a follow-up prompt: "We really need to go, don't we?". Now we evaluate the recovery, and we assess the LLM's subsequent response to see if it correctly recovers the intended implicature. Here, the corrected response should be "You're right, we should head home." Finally, we calculate the IRR.
- **Pragmatic Sensitivity Index (PSI)**: The PSI would measure the model’s sensitivity to subtle changes in context that should trigger different implicatures. For instance, changing a single word in a context could alter the expected implicature. PSI would assess whether the LLM correctly adjusts its output based on these subtle contextual shifts. For example, we plan to scramble nouns and replace key words with nonsense words, to examine the extent to which LLMs’ pragmatic sensitivity varies. 
